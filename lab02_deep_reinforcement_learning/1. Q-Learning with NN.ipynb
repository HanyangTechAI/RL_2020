{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Q-Learning with NN.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfoWO5Zxm9QV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./RL_2020\n",
        "!git clone https://github.com/HanyangTechAI/RL_2020.git\n",
        "!rm -rf ./RL_2020/.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ToR8Hl4m_Qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 필요한 패키지를 설치해준다.\n",
        "!cat ./RL_2020/lab02_deep_reinforcement_learning/requirements.txt\n",
        "!echo '----------------------------------'\n",
        "!pip install -r ./RL_2020/lab02_deep_reinforcement_learning/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPh7kWAhrRmH",
        "colab_type": "text"
      },
      "source": [
        "## 큐 함수를 근사하는 신경망\n",
        "CartPole-v0는 observation으로 네 개의 값이 들어오고, 에이전트가 할 수 있는 행동은 두 개 이다. 따라서 우리가 만들 신경망은 입력이 네 개이고 출력이 두 개여야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajbkCAijPeZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PyTorch를 사용하기 위한 모듈을 불러온다\n",
        "import torch\n",
        "from torch import nn, optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReS0nbQLPnDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 아래 ???을 채워봅시다.\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(QNetwork, self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(4, 16)\n",
        "    self.fc2 = nn.Linear(16, 2)\n",
        "\n",
        "    self.act = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.act(self.fc1(x))\n",
        "\n",
        "    return self.fc2(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ80OBbzCNcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Q-Learning를 이용한 에이전트를 만들자.\n",
        "class Agent:\n",
        "  def __init__(self, actions):\n",
        "    # 할 수 있는 행동의 집합\n",
        "    self.actions = actions\n",
        "\n",
        "    # 학습률로, 한 번에 얼마나 학습할지 결정한다.\n",
        "    self.learning_rate = 0.001\n",
        "\n",
        "    # 감가율\n",
        "    self.discount_factor = 0.9\n",
        "\n",
        "    # 무작위로 행동을 선택할 비율이다.\n",
        "    # 강화학습에선 탐험이 중요하기 때문에 무작위 선택도 넣는다.\n",
        "    self.epsilon = 0.1\n",
        "\n",
        "    # 큐 함수를 근사할 신경망이다.\n",
        "    self.q_net = QNetwork()\n",
        "\n",
        "    # 신경망을 최적화 할 optimizer\n",
        "    self.opt = optim.Adam(self.q_net.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    # 신경망을 최적화 할 때 사용할 loss function\n",
        "    self.criterion = nn.MSELoss()\n",
        "\n",
        "  # state를 받아 할 행동을 구하는 메소드\n",
        "  def get_action(self, state):\n",
        "    if random.random() < self.epsilon:\n",
        "      # 일정한 확률로 임의의 행동을 한다.\n",
        "      return self.actions.sample()\n",
        "    else:\n",
        "      q_values = self.q_net(state)\n",
        "      return q_values.argmax(dim=0).item()\n",
        "\n",
        "  # 보상을 바탕으로 Q Network를 업데이트 하는 메소드\n",
        "  def train(self, state, action, reward, next_state):\n",
        "    self.opt.zero_grad()\n",
        "\n",
        "    q1 = self.q_net(state)[action]\n",
        "    q2 = reward + self.discount_factor * torch.max(self.q_net(next_state).detach())\n",
        "\n",
        "    loss = self.criterion(q1, q2)\n",
        "    loss.backward()\n",
        "\n",
        "    self.opt.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4etWGID7vA2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "\n",
        "# 위에서 만든 에이전트를 바탕으로 학습을 시켜보자.\n",
        "env = gym.make('CartPole-v0')\n",
        "agent = Agent(env.action_space)\n",
        "\n",
        "# reward를 저장할 리스트\n",
        "rewards = []\n",
        "\n",
        "# 5000 에피소드를 수행한다.\n",
        "for episode in range(1, 5000 + 1):\n",
        "  state = env.reset()\n",
        "  \n",
        "  # state를 PyTorch의 Tensor형으로 변환해야한다.\n",
        "  state = torch.FloatTensor(state)\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  while True:\n",
        "    action = agent.get_action(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "    total_reward += reward\n",
        "\n",
        "    # next_state도 Tensor형으로 변환해야한다.\n",
        "    next_state = torch.FloatTensor(next_state)\n",
        "\n",
        "    if done:\n",
        "      rewards.append(total_reward)\n",
        "\n",
        "      reward = -1\n",
        "      agent.train(state, action, reward, next_state)\n",
        "      break\n",
        "    else:\n",
        "      agent.train(state, action, reward, next_state)\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "  # 이전 50개 에피소드의 평균 reward가 190보다 크거나 같으면 학습 중단\n",
        "  if np.mean(rewards[-min(50, len(rewards)):]) >= 190:\n",
        "    print('Early stopped (after {} episodes)'.format(episode))\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvFpXaRVY-oO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습이 진행됨에 따른 reward 변화를 그래프로 그린다.\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(range(1, len(rewards) + 1), rewards)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g85hqT9rtOk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}