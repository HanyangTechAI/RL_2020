{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. REINFORCE.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfoWO5Zxm9QV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./RL_2020\n",
        "!git clone https://github.com/HanyangTechAI/RL_2020.git\n",
        "!rm -rf ./RL_2020/.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ToR8Hl4m_Qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 필요한 패키지를 설치해준다.\n",
        "!cat ./RL_2020/lab02_deep_reinforcement_learning/requirements.txt\n",
        "!echo '----------------------------------'\n",
        "!pip install -r ./RL_2020/lab02_deep_reinforcement_learning/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPh7kWAhrRmH",
        "colab_type": "text"
      },
      "source": [
        "## 정책을 근사하는 신경망\n",
        "CartPole-v0은 observation으로 네 개의 값이 들어오고, 에이전트가 할 수 있는 행동은 두 개 이다. 따라서 우리가 만들 신경망은 입력이 네 개이고 출력이 두 개여야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajbkCAijPeZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PyTorch를 사용하기 위한 모듈을 불러온다\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReS0nbQLPnDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 아래 ???을 채워봅시다.\n",
        "\n",
        "class PolicyNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(PolicyNet, self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(4, 16)\n",
        "    self.fc2 = nn.Linear(16, 2)\n",
        "\n",
        "    self.act = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.act(self.fc1(x))\n",
        "\n",
        "    return F.softmax(self.fc2(x), dim=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ80OBbzCNcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# REINFORCE를 이용한 에이전트를 만들자.\n",
        "class Agent:\n",
        "  def __init__(self, actions):\n",
        "    # 할 수 있는 행동의 집합\n",
        "    self.actions = actions\n",
        "\n",
        "    # 학습률로, 한 번에 얼마나 학습할지 결정한다.\n",
        "    self.learning_rate = 0.01\n",
        "\n",
        "    # 감가율\n",
        "    self.discount_factor = 0.9\n",
        "\n",
        "    # 정책을 근사할 신경망이다.\n",
        "    self.p_net = PolicyNet()\n",
        "\n",
        "    # 신경망을 최적화 할 optimizer\n",
        "    self.opt = optim.Adam(self.p_net.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    # 학습 데이터를 저장할 리스트\n",
        "    self.log_probs = []\n",
        "    self.rewards = []\n",
        "\n",
        "  # state를 받아 할 행동을 구하는 메소드\n",
        "  def get_action(self, state):\n",
        "    policy = self.p_net(state)\n",
        "\n",
        "    m = Categorical(policy)\n",
        "    action = m.sample()\n",
        "\n",
        "    self.log_probs.append(m.log_prob(action))\n",
        "\n",
        "    return action.item()\n",
        "\n",
        "  # 보상을 바탕으로 Q Network를 업데이트 하는 메소드\n",
        "  def train(self):\n",
        "    R = 0\n",
        "    policy_loss = []\n",
        "    returns = []\n",
        "\n",
        "    for r in self.rewards[::-1]:\n",
        "      R = r + self.discount_factor * R\n",
        "      returns.insert(0, R)\n",
        "\n",
        "    returns = torch.tensor(returns)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
        "\n",
        "    for log_prob, R in zip(self.log_probs, returns):\n",
        "      policy_loss.append(-log_prob * R)\n",
        "\n",
        "    self.opt.zero_grad()\n",
        "\n",
        "    loss = sum(policy_loss)\n",
        "    loss.backward()\n",
        "\n",
        "    self.opt.step()\n",
        "\n",
        "    self.log_probs.clear()\n",
        "    self.rewards.clear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4etWGID7vA2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "\n",
        "# 위에서 만든 에이전트를 바탕으로 학습을 시켜보자.\n",
        "env = gym.make('CartPole-v0')\n",
        "agent = Agent(env.action_space)\n",
        "\n",
        "# reward를 저장할 리스트\n",
        "rewards = []\n",
        "\n",
        "# 5000 에피소드를 수행한다.\n",
        "for episode in range(1, 5000 + 1):\n",
        "  state = env.reset()\n",
        "  \n",
        "  # state를 PyTorch의 Tensor형으로 변환해야한다.\n",
        "  state = torch.FloatTensor(state)\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  while True:\n",
        "    action = agent.get_action(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "    agent.rewards.append(reward)\n",
        "    total_reward += reward\n",
        "\n",
        "    # next_state도 Tensor형으로 변환해야한다.\n",
        "    next_state = torch.FloatTensor(next_state)\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "  rewards.append(total_reward)\n",
        "  agent.train()\n",
        "\n",
        "  # 이전 50개 에피소드의 평균 reward가 190보다 크거나 같으면 학습 중단\n",
        "  if np.mean(rewards[-min(50, len(rewards)):]) >= 190:\n",
        "    print('Early stopped (after {} episodes)'.format(episode))\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvFpXaRVY-oO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습이 진행됨에 따른 reward 변화를 그래프로 그린다.\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(range(1, len(rewards) + 1), rewards)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g85hqT9rtOk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}