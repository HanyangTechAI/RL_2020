{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Introduction to Reinforcement Learning with OpenAI Gym-checkpoint.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd6GdUs0j8lq",
        "colab_type": "text"
      },
      "source": [
        "## 강화학습(Reinforcement Learning)이란?\n",
        "위키피디아에 따르면 강화학습의 정의는 다음과 같다.\n",
        ">어떤 환경 안에서 정의된 에이전트가 현재의 상태를 인식하여, 선택 가능한 행동 중 보상을 최대화하는 행동 혹은 행동 순서를 선택하는 방법\n",
        "\n",
        "강화학습은 **정답이나 잘못된 선택에 대한 정정이 주어지지 않는** 특징이 있다. 그렇기에 MDP(Markov Decision Process)를 정의해 강화학습 문제를 푼다.\n",
        "\n",
        "### 예시\n",
        "- [AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far)\n",
        "- [AlphaStar](https://deepmind.com/research/publications/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra-aUqUsswrk",
        "colab_type": "text"
      },
      "source": [
        "## MDP(Markov Decision Process)\n",
        "MDP는 다음 구성요소를 가진다.\n",
        "- 상태 (State)\n",
        "- 행동 (Action)\n",
        "- 보상 (Reward)\n",
        "- 상태 변환 확률 (State Transition Probability)\n",
        "- 감가율 (Discount Factor)\n",
        "\n",
        "강화학습 문제를 풀 땐 MDP를 잘 정의하는 게 우선이다. MDP의 각 요소가 무엇인지는 이제 직접 환경을 보며 알아보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK-M4eB9TP5f",
        "colab_type": "text"
      },
      "source": [
        "## OpenAI Gym\n",
        "강화학습엔 환경(environment)이 필요하다. OpenAI Gym는  다양한 강화학습 환경을 제공한다.\n",
        "\n",
        "이제 OpenAI Gym을 설치해 본격적으로 강화학습을 실습해보자.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMevRkF7YwuC",
        "colab_type": "code",
        "outputId": "b1c95401-20bc-4ae2-a773-4d91b4245709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "# 필요한 패키지를 설치해준다.\n",
        "!cat requirements.txt\n",
        "!echo '----------------------------------'\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gym\r\n",
            "numpy\r\n",
            "matplotlib\r\n",
            "----------------------------------\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.15.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.17.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->-r requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->-r requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym->-r requirements.txt (line 1)) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->-r requirements.txt (line 3)) (42.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NImuLvpYa2y6",
        "colab_type": "text"
      },
      "source": [
        "## Frozen Lake\n",
        "처음으로 실습할 환경은 ***Frozen Lake***다. Frozen Lake는 에이전트가 구멍을 피해 얼어있는 강을 건너는 환경이다.\n",
        "\n",
        "판의 크기는 4 by 4로 총 16칸이며, 에이전트는 상하좌우로 한 칸 이동할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z93Sx8FnY9hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 다음 코드를 통해 FrozenLake 환경을 불러올 수 있다.\n",
        "import gym\n",
        "\n",
        "env = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mprDLfVaEkX",
        "colab_type": "code",
        "outputId": "9106b37d-9a58-4f68-9afb-3255b9e68e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# 환경에 관한 정보는 다음과 같이 얻을 수 있다.\n",
        "\n",
        "print('Observation Space:', env.observation_space)\n",
        "print('Action Space:', env.action_space)\n",
        "\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation Space: Discrete(16)\n",
            "Action Space: Discrete(4)\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfBlEi2P3mBC",
        "colab_type": "code",
        "outputId": "8f9d927e-9fca-4ea4-89e6-d07a58fa66ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# episode를 시작하기 전엔 reset 메소드를 호출해야한다.\n",
        "# reset 메소드는 state를 반환한다.\n",
        "state = env.reset()\n",
        "print(state)\n",
        "\n",
        "# step 메소드를 이용해 한 스텝 진행할 수 있다.\n",
        "# step 메소드는 새로운 state, 보상, 끝났는지 여부, 기타 정보를 반환한다.\n",
        "next_state, reward, done, _ = env.step(env.action_space.sample())\n",
        "print('next_state:', next_state)\n",
        "print('reward:', reward)\n",
        "print('done:', done)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "next_state: 1\n",
            "reward: 0.0\n",
            "done: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue4hyl09awtI",
        "colab_type": "code",
        "outputId": "9daac86b-49b8-4696-b97f-83b1dd8ac09b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 지금까지 배운 내용으로 무작위로 행동하는 에이전트를 만들어보자.\n",
        "\n",
        "# 환경을 만든다\n",
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "# Episode를 시작하기 전엔 반드시 reset 메소드를 호출해야한다.\n",
        "state = env.reset()\n",
        "\n",
        "while True:\n",
        "  # 환경을 출력한다.\n",
        "  env.render()\n",
        "\n",
        "  # Action space에서 무작위 행동을 가져온다.\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "  # 생성한 action을 바탕으로 한 스텝 진행한다.\n",
        "  new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "  # Episode가 끝났다면 반복문을 빠져나간다.\n",
        "  if done:\n",
        "    print(reward)\n",
        "    break\n",
        "\n",
        "  # state에 새로운 state를 할당한다. (지금은 의미 없음)\n",
        "  state = new_state\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej_d2Fi93w8X",
        "colab_type": "text"
      },
      "source": [
        "## Q Learning\n",
        "어떤 상황 *s*에서 어떤 행동 *a*의 가치를 Q(s, a)라 하자. 그렇다면 가장 좋은 행동은 Q(s, a)가 가장 큰 *a*이다.\n",
        "\n",
        "그렇다면 Q 값은 어떻게 구할까? 보상(reward)를 바탕으로 Q 값을 갱신하면 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ80OBbzCNcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}