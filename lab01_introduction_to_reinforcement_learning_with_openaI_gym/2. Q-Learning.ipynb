{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Q-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfoWO5Zxm9QV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./RL_2020\n",
        "!git clone https://github.com/HanyangTechAI/RL_2020.git\n",
        "!rm -rf ./RL_2020/.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ToR8Hl4m_Qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 필요한 패키지를 설치해준다.\n",
        "!cat ./RL_2020/lab01_introduction_to_reinforcement_learning_with_openaI_gym/requirements.txt\n",
        "!echo '----------------------------------'\n",
        "!pip install -r ./RL_2020/lab01_introduction_to_reinforcement_learning_with_openaI_gym/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej_d2Fi93w8X",
        "colab_type": "text"
      },
      "source": [
        "## Q Learning\n",
        "어떤 상황 s에서 어떤 행동 a의 가치를 Q(s, a)라 하자. 그렇다면 가장 좋은 행동은 Q(s, a)가 가장 큰 a이다. 그렇다면 Q 값은 어떻게 구할까? 보상(reward)를 바탕으로 Q 값을 갱신하면 된다.\n",
        "\n",
        "다음의 수식에 따라 Q값을 갱신하는 것을 Q-Learning이라 한다.\n",
        "\n",
        "$$Q(s,a) ←Q(s,a) + \\alpha \\left [ r + \\gamma max_{a'} Q(s', a') - Q(s,a)\\right]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWLjZIE_sYCv",
        "colab_type": "text"
      },
      "source": [
        "### e-greedy\n",
        "강화학습에선 탐험하는 것이 중요하다. 이번 시간엔 epsilon greedy 알고리즘을 이용해 에이전트가 탐험하도록 할 것이다.\n",
        "\n",
        "e-greedy 알고리즘의 작동 방식은 다음과 같다.\n",
        "- 일정한 확률로 임의의 행동을 선택한다.\n",
        "- 그 외에는 가장 좋은 행동을 선택한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ80OBbzCNcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Q-Learning를 이용한 에이전트를 만들자.\n",
        "class Agent:\n",
        "  def __init__(self, actions):\n",
        "    # 할 수 있는 행동의 집합\n",
        "    self.actions = actions\n",
        "\n",
        "    # 학습률로, 한 번에 얼마나 학습할지 결정한다.\n",
        "    self.learning_rate = 0.85\n",
        "\n",
        "    # 감가율\n",
        "    self.discount_factor = 0.9\n",
        "\n",
        "    # 무작위로 행동을 선택할 비율이다.\n",
        "    # 강화학습에선 탐험이 중요하기 때문에 무작위 선택도 넣는다.\n",
        "    self.epsilon = 0.9\n",
        "\n",
        "    # Q(s, a)를 담아둔 테이블이다\n",
        "    self.q_table = defaultdict(lambda: [0. for _ in range(actions.n)])\n",
        "\n",
        "  # state를 받아 할 행동을 구하는 메소드\n",
        "  def get_action(self, state):\n",
        "    if random.random() < self.epsilon:\n",
        "      # 일정한 확률로 임의의 행동을 한다.\n",
        "      return self.actions.sample()\n",
        "    else:\n",
        "      q_values = self.q_table[state]\n",
        "      return np.argmax(q_values)\n",
        "\n",
        "  # 보상을 바탕으로 Q table을 업데이트 하는 메소드\n",
        "  def train(self, state, action, reward, next_state):\n",
        "    q1 = self.q_table[state][action]\n",
        "    q2 = reward + self.discount_factor * max(self.q_table[next_state])\n",
        "\n",
        "    self.q_table[state][action] += self.learning_rate * (q2 - q1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4etWGID7vA2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "\n",
        "# 위에서 만든 에이전트를 바탕으로 학습을 시켜보자.\n",
        "# 다만, 문제를 조금 쉽게 하기 위해 미끄럼 방지를 켜둔다.\n",
        "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
        "agent = Agent(env.action_space)\n",
        "\n",
        "# 시간에 따른 보상의 변화를 보기 위해 받은 보상을 저장해두자.\n",
        "rewards = []\n",
        "\n",
        "# 3000 에피소드를 수행한다.\n",
        "for episode in range(1, 3000 + 1):\n",
        "  state = env.reset()\n",
        "\n",
        "  episode_rewards = []\n",
        "  while True:\n",
        "    action = agent.get_action(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "    if done:\n",
        "      # 만약 죽었다면 음의 보상을 준다.\n",
        "      if reward == 0:\n",
        "        reward = -1\n",
        "\n",
        "      agent.train(state, action, reward, next_state)\n",
        "      episode_rewards.append(reward)\n",
        "      break\n",
        "    else:\n",
        "      agent.train(state, action, reward, next_state)\n",
        "      episode_rewards.append(reward)\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "  rewards.append(sum(episode_rewards) / len(episode_rewards))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU5-f7rrxtSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 에이전트가 정상적으로 학습이 됐는지 확인하기 위해 epsilon을 0으로 하고 에피소드를 수행해보자.\n",
        "state = env.reset()\n",
        "agent.epsilon = 0\n",
        "\n",
        "while True:\n",
        "  env.render()\n",
        "\n",
        "  print(agent.q_table[state])\n",
        "\n",
        "  action = agent.get_action(state)\n",
        "  next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "  state = next_state\n",
        "\n",
        "  if done:\n",
        "    if reward > 0:\n",
        "      print('Solved!')\n",
        "    else:\n",
        "      print('Failed')\n",
        "\n",
        "    env.render()\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvFpXaRVY-oO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}